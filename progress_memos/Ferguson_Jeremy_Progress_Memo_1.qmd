---
title: "Progress Memo 1"
subtitle: |
  | Final Project 
  | Data Science 1 with R (STAT 301-1)
author: "Jeremy Ferguson"
date: today

format:
  html:
    toc: true
    code-fold: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
---

::: {.callout-tip icon=false}

## Github Repo Link

[https://github.com/stat301-1-2023-fall/final-project-1-jeremyferg.git](https://github.com/stat301-1-2023-fall/final-project-1-jeremyferg.git)

:::

## Datasets Used

```{r}
#| label: loading libraries
#| echo: true
#| code-fold: show

#loading packages
library(tidyverse)
library(rvest)
```


## Project Purpose

In my project, I will analyze the correlation between stocks of video game publishers and industry-related events occurring over the past 10 years. Events in this analysis are defined as any media occurrences that contribute to the popularity of a publisher's game (ex: game review, game awards, etc.). I will examine the effect a publisher’s own events have on their stock and the effects events related to the publisher have on their stock (ex: a game release from an other publisher specialized in the same game genre). I will analyze these correlations by creating a time series dataset that records stocks and events per day. 

## Data source

My data source will consist of a mixture of existing datasets and data I gather through web scraping. I ultimately plan to create a time series data set, where between Jun. 2013 and Jun. 2023, I record the stock of the video game publisher and any events I’ve identified occurring on that date that could alter stocks. The events I intend to focus on are game release dates, reviews, esports earnings/tournaments, and game awards. 

I will retrieve stock data on [Yahoo Finance](https://finance.yahoo.com/). These datasets will have 6 columns excluding the date column. I plan to compare 10 different publisher’s stocks: [Nintendo](https://finance.yahoo.com/quote/NTDOY/), [EA](https://finance.yahoo.com/quote/EA/history?period1=1370044800&period2=1685577600&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true), [Activision Blizzard](https://finance.yahoo.com/quote/ATVI/history?period1=1370044800&period2=1685577600&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true), [Ubisoft](https://finance.yahoo.com/quote/UBI.PA/history?period1=1370044800&period2=1685577600&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true), [Square Enix](https://finance.yahoo.com/quote/9684.T/history?period1=1370044800&period2=1685577600&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true), [Bandai Namco](https://finance.yahoo.com/quote/7832.T/history?period1=1370044800&period2=1685577600&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true), [Sega](https://finance.yahoo.com/quote/6460.T/history?period1=1370044800&period2=1685577600&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true), [Take-Two](https://finance.yahoo.com/quote/TTWO/history?period1=1370044800&period2=1685577600&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true), [Capcom](https://finance.yahoo.com/quote/9697.T/history?period1=1370044800&period2=1685577600&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true), and [Konami](https://finance.yahoo.com/quote/9766.T/history?period1=1370044800&period2=1685577600&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true). 

I found a few datasets that recorded video game release dates, but these datasets were either missing games or were published years ago. So, I will scrape this information from [Wikipedia](https://en.wikipedia.org/wiki/Category:Video_games_by_year). I plan to web scrape game names, game series, publishers, release dates, genre of game, and game modes. I’ve created a small example of what this dataset would look like using web scraping (shown in the data quality section). The goal, of course, is to create a function that can make this dataset at once. 

I found the same dataset issue when looking at data for game awards and game reviews. Concerning game awards, I will be scraping Wikipedia pages for [The Game Awards results](https://en.wikipedia.org/wiki/The_Game_Awards). The Game Awards is the annual, most acclaimed awards ceremony in the video game industry. I am currently planning to have the date of the award, game name, award category, publisher, and win (whether or not the nominee won) as the collected data. Each observation in this dataset will be a nominee, so there is no need for a nominee column. The number of nominees and wins a game/publisher receives will also prove to be useful in the analysis. 

I will be web-scraping video game review information from [Metacritic](https://www.metacritic.com/), which hosts an aggregate set of game reviews similar to Rotten Tomatoes. My intended columns are game reviewed, review date, critic or user (TRUE if the review was done by a critic), score, average critic score yesterday ( average score of all critics before the observation’s review date), and average user score yesterday (similar meaning).

Finally, I was able to find an extensive, up-to-date dataset of [esports-related games and earnings from tournaments](https://www.kaggle.com/datasets/rankirsh/esports-earnings). Observations are recorded every month (instead of daily like my other data). Along with the monthly date, the dataset’s columns are game, monthly earnings, number of players getting a share of the earnings, and the number of tournaments that month. I may web scrape the data’s [original site](https://www.esportsearnings.com/) to get daily records, as exact tournament dates and earnings are made public. However, I believe I can produce good information concerning esports with monthly records and currently do not intend on scraping this site. 

## Why this data

My main goal with choosing this dataset is to become better at tidying data and learning how to web scrape. I thought it would be a nice challenge to go beyond what we have done in lecture and improve these skills independently. I chose the video game industry to achieve these goals because I have some familiarity with many of the companies in the data. This initial familiarity makes identifying potential variables worth analyzing much easier. Moreover, I want to explore what components of the industry most affect a company’s success (more so out of curiosity). 

## Data quality & complexity check

The complexity of my data comes from the web scraping and merging I will do to create my final time-series data. Here is an example of me reading in stock data and esports data:

```{r}
#| label: stock-readin

#reading in each company's data
ntdoy <- read_csv('data/publisher_stocks/NTDOY.csv')
ea <- read_csv('data/publisher_stocks/EA.csv')
atvi <- read_csv('data/publisher_stocks/ATVI.csv')
ubi <- read_csv('data/publisher_stocks/UBI.PA.csv')
sqenix <- read_csv('data/publisher_stocks/SQENIX.T.csv')
sega <- read_csv('data/publisher_stocks/SEGA.T.csv')
ttwo <- read_csv('data/publisher_stocks/TTWO.csv')
bandai <- read_csv('data/publisher_stocks/BANNAM.T.csv')
capcom <- read_csv('data/publisher_stocks/CAPCOM.T.csv')
konami <- read_csv('data/publisher_stocks/KONA.T.csv')

#ubi data had differing column types
#need same colum types for bind_rows()
ubi <- ubi |> 
  mutate(Open = as.numeric(Open),
         High = as.numeric(High),
         Low = as.numeric(Low),
         Close = as.numeric(Close),
         `Adj Close` = as.numeric(`Adj Close`),
         Volume = as.numeric(Volume))

#stacking observations on top of each other
bind_rows(ntdoy, ea, atvi, ubi, sqenix, sega, ttwo, bandai, capcom, konami)

```

The stock data has one column of type date and six columns of type dbl. Dates are treated as numerical variables in this analysis. Stocks are not perfectly recorded every day (an observation I will have to look into further), but I end up with 22,649 observations when combining all stock datasets.

```{r}
#| label: esport-readin

historical_esport_data <- read_csv('data/HistoricalEsportData.csv')

historical_esport_data
```

The esports data has one column of type date, three columns of type double, and one column of type character. The dataset has 9,244 observations.

Here is an example of web scraping data for game-release information:

```{r}
#| label: webscrape-ex
#| echo: True

#### cleaning up EA Sports FC 2024 ----

html_ea_sports_fc_24 <- read_html('https://en.wikipedia.org/wiki/EA_Sports_FC_24')

##getting the table and the desired columns
new_column_names <- c('x', 'y')

reading_ea_sports_fc_24 <- 
  html_ea_sports_fc_24 |> 
  html_element('.infobox') |> 
  html_table() |> 
  janitor::clean_names() |> 
  rename_all(~new_column_names) |> 
  filter(x == 'Publisher(s)' | 
           x == 'Series' | 
           x == 'Release' |  
           x == 'Genre(s)' |
           x == 'Mode(s)') |> 
  add_row(x = 'Name', y = 'EA_Sports_FC_24', .before = 1)

##using pivot_wider to get each row its own column
reading_ea_sports_fc_24 <- reading_ea_sports_fc_24 |> 
  pivot_wider(
    names_from = 'x',
    values_from = 'y') |> 
  janitor::clean_names() |> 
  mutate(release = as.Date(release, '%d %B %Y'))

##there were two values in mode_s, expanding to two rows with this code
reading_ea_sports_fc_24 <- reading_ea_sports_fc_24 |> 
  separate_rows(mode_s, sep = ", ")

#### cleaning up Alan Wake II ----

#trying another game, code from previous read-in is nearly identical 
reading_alan_wake_ii <- read_html('https://en.wikipedia.org/wiki/Alan_Wake_II')

##getting the table and the desired columns
new_column_names <- c('x', 'y')

reading_alan_wake_ii <- 
  reading_alan_wake_ii |> 
  html_element('.infobox') |> 
  html_table() |> 
  janitor::clean_names() |> 
  rename_all(~new_column_names) |> 
  filter(x == 'Publisher(s)' | 
           x == 'Series' | 
           x == 'Release' |  
           x == 'Genre(s)' |
           x == 'Mode(s)') |> 
  add_row(x = 'Name', y = 'Alan_Wake_II', .before = 1)

##using pivot_wider to get each row its own column
reading_alan_wake_ii <- reading_alan_wake_ii |> 
  pivot_wider(
    names_from = 'x',
    values_from = 'y'
  ) |> 
  janitor::clean_names() |> 
  mutate(release = as.Date(release, '%d %B %Y'))

##there were two values in mode_s, expanding to two rows with this code
reading_alan_wake_ii <- reading_alan_wake_ii |> 
  separate_rows(mode_s, sep = ", "
  )

bind_rows(reading_ea_sports_fc_24, reading_alan_wake_ii)
```

The tibble has one column of type date and five columns of type character. Based on game entries in Wikipedia, I expect to create around 5,500 observations.

The Game Awards dataset will have one column of type date (the date), one column of type boolean (won the award or not), and three columns of type character (game name, award category, publisher). Only a small group of games are nominated for awards, so I expect this dataset to have an observation length in the 100s. The game reviews dataset will have one column of type date (the date), three columns of type double (score, average critic/user score yesterday), one column of type boolean (critic or user), and one column of type character (game name). I estimate this dataset to have observations in the 1,000s or 10,000s.

Many of these datasets share similar columns like game names and dates. Even so, I believe my collection of columns and observations meets the complexity requirements when considering how I collected my data.

## Potential data issues

Concerning potential data issues, web scraping and merging datasets could create the most problems. Again, I am learning how to web scrape for this project, meaning there is potential of me getting stuck on how to scrape specific information from sites. Creating a well-organized merged table may also prove to be a bit difficult. I may still have missing data after web scraping (ex: there may be games with no reviews, making the review score variable N/A) I will mostly have to create a publisher index/primary key to accomplish the merging. 

## Misc

While I’m challenging myself through the methods I retrieve data, the main task of the assignment is to create an EDA. The EDA part of my project may be more leniently graded compared to others, but I should make sure to still create sufficient time to perform a thorough analysis of my data.
